{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the results of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils) # Reload instead of using cached version\n",
    "\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "\n",
    "# Make sure we're using python 3.10.1 (same as version on short)\n",
    "!python3 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set root to git subfolder\n",
    "git_root = subprocess.check_output(['git', 'rev-parse', '--show-toplevel']).decode('utf-8').strip()\n",
    "os.chdir(git_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = \"experiments/mini_experiments/results_triangle_sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first extract all the experiments and trial (folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_root_name = lambda path : path.split(\"/\")[-2]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Loop through all the search algorithms\n",
    "search_algorithms = glob.glob(RESULT_DIR + \"/*/\")\n",
    "for search_algorithm in search_algorithms:\n",
    "    rl_algorithms = glob.glob(search_algorithm + \"/*/\")\n",
    "    for rl_algorithm in rl_algorithms:\n",
    "        trial_numbers = glob.glob(rl_algorithm + \"/*/\")\n",
    "        for trial_number in trial_numbers:\n",
    "            path_log = trial_number + \"logs/\"\n",
    "            new_row = pd.DataFrame([{\"search_algorithm\": get_root_name(search_algorithm), \"rl_algorithm\": get_root_name(rl_algorithm), \"trial_number\": get_root_name(trial_number).split(\"_\")[-1], \"path_log\": path_log}])\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns in tvec.csv: \n",
    "# algo_name,algo_name_nice,scenario_name,scenario_name_nice,seed,k,patch,patch_clean,patch_valid,diff,diff_valid,stats_steps,stats_steps_useful,stats_steps_useful_105,log_time_search,log_time_validation,log_time_test,log_time_valid_test,log_time_all,log_time_valid_all,fit_search,fit_training,fit_validation,fit_test,fit_all,fit_valid_training,fit_valid_validation,fit_valid_test,fit_valid_all,fit_init_search,fit_init_training,fit_init_validation,fit_init_test,fit_init_all,fit_init_valid_training,fit_init_valid_validation,fit_init_valid_test,fit_init_valid_all,runtime_test,runtime_all,runtime_valid_test,runtime_valid_all,runtime_init_test,runtime_init_all,runtime_init_valid_test,runtime_init_valid_all,patch_size,patch_clean_size,patch_valid_size,ratio_fit_search,ratio_fit_training,ratio_fit_validation,ratio_fit_test,ratio_fit_all,ratio_fit_valid_training,ratio_fit_valid_validation,ratio_fit_valid_test,ratio_fit_valid_all,ratio_runtime_test,ratio_runtime_all,ratio_runtime_valid_test,ratio_runtime_valid_all,ratio_steps_useful,ratio_steps_useful_105\n",
    "# TODO: stats_steps,stats_steps_useful,stats_steps_useful_105,log_time_search,log_time_validation,log_time_test,log_time_valid_test,log_time_all,log_time_valid_all,fit_search,fit_training,fit_validation,fit_test,fit_all,fit_valid_training,fit_valid_validation,fit_valid_test,fit_valid_all,fit_init_search,fit_init_training,fit_init_validation,fit_init_test,fit_init_all,fit_init_valid_training,fit_init_valid_validation,fit_init_valid_test,fit_init_valid_all,runtime_test,runtime_all,runtime_valid_test,runtime_valid_all,runtime_init_test,runtime_init_all,runtime_init_valid_test,runtime_init_valid_all,patch_size,patch_clean_size,patch_valid_size,ratio_fit_search,ratio_fit_training,ratio_fit_validation,ratio_fit_test,ratio_fit_all,ratio_fit_valid_training,ratio_fit_valid_validation,ratio_fit_valid_test,ratio_fit_valid_all,ratio_runtime_test,ratio_runtime_all,ratio_runtime_valid_test,ratio_runtime_valid_all,ratio_steps_useful,ratio_steps_useful_105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Pickle Object\n",
    "df['pkl_obj'] = df['path_log'].apply(utils.generate_pickle_object)\n",
    "# df['pkl_obj'].iloc[0] # Example of what the pickle object looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract some common columns we might need\n",
    "\n",
    "# df['seed'] = df['pkl_obj'].apply(lambda x: x['seed'])\n",
    "df['diff'] = df['pkl_obj'].apply(lambda x: x['diff'])\n",
    "df['initial_fitness'] = df['pkl_obj'].apply(lambda x: x['initial_fitness'])\n",
    "df['best_fitness'] = df['pkl_obj'].apply(lambda x: x['best_fitness'])\n",
    "df['patch'] = df['path_log'].apply(utils.get_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_variants_evaluated'] = df['pkl_obj'].apply(utils.get_num_variants_evaluated) \n",
    "df['num_successful_variants_evaluated'] = df['pkl_obj'].apply(utils.get_num_successful_variants_evaluated)\n",
    "df['get_unique_statuses'] = df['pkl_obj'].apply(utils.get_unique_statuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fitness_decrease_percentage'] = (df['best_fitness'] / df['initial_fitness']) * 100\n",
    "df['successful_runs_percentage'] = (df['num_successful_variants_evaluated'] / df['num_variants_evaluated']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['operator_selector'] = df['pkl_obj'].apply(lambda x: x['operator_selector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop([3]) #Â Just for now\n",
    "df['rewards'] = df['operator_selector'].apply(lambda x: np.array(x.reward_log))\n",
    "df['cumulative_rewards'] = df['rewards'].apply(lambda x: np.cumsum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we explore the variance of the warmup phase. For our experiment, we used to perf function in hopes of minimizing the variance of the warmup phase. The hope is that the variance of the warmup phase is small enough that we can ignore it. We will explore this assumption here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['warmup_values'] = df['pkl_obj'].apply(lambda x: np.array(x['warmup_values'])) \n",
    "\n",
    "((df['warmup_values'] / df['warmup_values'].apply(lambda x: np.median(x))) - 1) * 100 # Percentage difference from median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the perf function creates values that are very close together. However, there are some significant outliers. We create a boxplot to better visualize the variance of the warmup phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a boxplot for each experiment\n",
    "plt.boxplot([df['warmup_values'].iloc[i] for i in range(len(df))])\n",
    "plt.title('Number of instruction for each experiment')\n",
    "plt.ylabel('Number of instructions')\n",
    "plt.xlabel('Experiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each experiment should execute the same number of instructions, so we can combine all the warmup trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_np_array = np.concatenate([df['warmup_values'].iloc[i] for i in range(len(df))])\n",
    "plt.boxplot(combined_np_array)\n",
    "plt.title('Number of instruction for each experiment')\n",
    "plt.ylabel('Number of instructions')\n",
    "plt.xlabel('Combined')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also normalise by the minimum value\n",
    "percentage_diff_median = combined_np_array / combined_np_array.min()\n",
    "plt.boxplot(percentage_diff_median)\n",
    "plt.title('Minimum normalised number of instruction')\n",
    "plt.ylabel('Ratio of minimum value')\n",
    "plt.xlabel('Combined')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the variance is mostly consistance, however, at times the number of instructions exectued jump up very significantly. We will explore this further in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers():\n",
    "    # Get outliers\n",
    "    Q1 = np.quantile(combined_np_array, 0.25)\n",
    "    Q3 = np.quantile(combined_np_array, 0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = combined_np_array[(combined_np_array < Q1 - 1.5 * IQR) | (combined_np_array > Q3 + 1.5 * IQR)]\n",
    "    return outliers\n",
    "\n",
    "outliers = get_outliers()\n",
    "\n",
    "print(f\"There are {len(outliers)} outliers, so {len(outliers) * 100 / len(combined_np_array):.2f}% of the data is outliers\")\n",
    "\n",
    "# Percentage difference from median\n",
    "average_outlier_magnitude = ((outliers / np.median(combined_np_array)) - 1) * 100\n",
    "if len(average_outlier_magnitude) > 0:\n",
    "    print(f\"Average outlier magnitude: {np.mean(average_outlier_magnitude):.2f}%\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This essentially gives use evidence that parallisation will induce very significant noise. As context switching leads to an uptick in instruction count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse experiment runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply compare operators between each other for random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rl_algos = df['rl_algorithm'].unique()\n",
    "unique_rl_algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rl_algos_as_box_plot(column_name, title, ylabel):\n",
    "    box_mini_plot = []\n",
    "    for rl_algo_name in unique_rl_algos:\n",
    "        rl_algo = df[df['rl_algorithm'] == rl_algo_name]\n",
    "\n",
    "        values = np.array(rl_algo[column_name])\n",
    "        box_mini_plot.append(values)\n",
    "    plt.boxplot(box_mini_plot, labels=unique_rl_algos)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficacy\n",
    "# compare_rl_algos_as_box_plot('best_fitness', 'Best fitness for each RL algorithm', 'Best fitness (# instructions)')\n",
    "compare_rl_algos_as_box_plot('fitness_decrease_percentage', 'Best fitness decrease percentage for each RL algorithm', 'Fitness decrease percentage (%)')\n",
    "\n",
    "# Efficiency\n",
    "compare_rl_algos_as_box_plot('successful_runs_percentage', 'Percentage of successful runs for each RL algorithm', 'Percentage of successful runs (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(ax, series, series_name):\n",
    "    time_steps = list(range(len(series)))\n",
    "    ax.plot(time_steps, series)\n",
    "    ax.set_xlabel('Time Steps')\n",
    "    ax.set_ylabel(series_name)\n",
    "    ax.set_title(f'{series_name} over Time')\n",
    "\n",
    "def plot_rewards_vs_time(ax, op_selector):\n",
    "    plot_time_series(ax, op_selector.reward_log, 'Rewards')\n",
    "\n",
    "def plot_quality_vs_time(ax, op_selector):\n",
    "    for operator in op_selector._operators:\n",
    "        ax.plot([average_qualities[operator] for average_qualities in op_selector.average_qualities_log], label=f\"{operator.__name__}\")\n",
    "    ax.set_xlabel('Time Steps')\n",
    "    ax.set_ylabel('Quality')\n",
    "    ax.set_title('Quality over Time')\n",
    "    ax.legend()\n",
    "\n",
    "def plot_count_vs_time(ax, op_selector):\n",
    "    for operator in op_selector._operators:\n",
    "        ax.plot([action_count[operator] for action_count in op_selector.action_count_log], label=f\"{operator.__name__}\")\n",
    "    ax.set_xlabel('Time Steps')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Count over Time')\n",
    "    ax.legend()\n",
    "\n",
    "for rl_algo_name in unique_rl_algos:\n",
    "\n",
    "    # if rl_algo_name == 'UniformSelector':\n",
    "    #     continue\n",
    "    \n",
    "    print(f\"Results for trials of {rl_algo_name}\")\n",
    "    df_rl = df[df['rl_algorithm'] == rl_algo_name]\n",
    "\n",
    "    num_rows = len(df_rl)\n",
    "\n",
    "    fig, axs = plt.subplots(num_rows, 3, figsize=(40, 8 * num_rows)) \n",
    "    if num_rows == 1: # Prevent single dimension error\n",
    "        axs = np.array([axs])\n",
    "\n",
    "    for trial_num in range(num_rows):\n",
    "        op_selector = df_rl[df_rl['trial_number'] == str(trial_num)].iloc[0]['pkl_obj']['operator_selector']\n",
    "        plot_rewards_vs_time(axs[trial_num, 0], op_selector)\n",
    "        plot_quality_vs_time(axs[trial_num, 1], op_selector)\n",
    "        plot_count_vs_time(axs[trial_num, 2], op_selector)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_vs_time(col_name, ylabel, title):\n",
    "    colors = iter(plt.cm.rainbow(np.linspace(0, 1, len(df['rl_algorithm'].unique()))))\n",
    "\n",
    "    for rl_algorithm in df['rl_algorithm'].unique():\n",
    "        color = next(colors)\n",
    "        df_algorithm = df[df['rl_algorithm'] == rl_algorithm]\n",
    "\n",
    "        # Loop over the first n rows of df_algorithm\n",
    "        n = len(df_algorithm)\n",
    "        for i in range(n):\n",
    "            if i == 0:\n",
    "                # Plot the first line and set a label for the legend\n",
    "                plt.plot(df_algorithm[col_name].iloc[i], color=color, label=rl_algorithm)\n",
    "            else:\n",
    "                # Plot the remaining lines without a label to avoid duplicate labels in the legend\n",
    "                plt.plot(df_algorithm[col_name].iloc[i], color=color)\n",
    "\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumulative_vs_time(\"cumulative_rewards\", \"Cumulative Reward\", \"Cumulative Reward over Time Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_statuses = reduce(set.union, df['get_unique_statuses'])\n",
    "unique_statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the relevant columns\n",
    "for unique_status in unique_statuses:\n",
    "    df[f\"{unique_status}_count_over_time\"] = df['pkl_obj'].apply(lambda x: utils.get_status_cum_count(x, unique_status))\n",
    "\n",
    "for unique_status in unique_statuses:\n",
    "    plot_cumulative_vs_time(f\"{unique_status}_count_over_time\", f\"{unique_status} Count\", f\"{unique_status} Count over Time\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magpie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
